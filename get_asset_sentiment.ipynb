{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_asset_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtHfF1fjUbfMmCUP4KQ8T6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhildaphara/automate-stock/blob/main/get_asset_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys5RCVc1eKHB"
      },
      "source": [
        "1. Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI6tV3yzjjxo"
      },
      "source": [
        "!pip install bs4\n",
        "!pip install sentencepiece \n",
        "!pip install transformers[torch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbkQRPxzkR63"
      },
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNLff4DzeM9g"
      },
      "source": [
        "2. Initiate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMFataAPlg5h"
      },
      "source": [
        "model_name = \"human-centered-summarization/financial-summarization-pegasus\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name) "
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2cjIBotnrjP"
      },
      "source": [
        "3. Define Tickers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhA13QWsnrIA"
      },
      "source": [
        "monitored_tickers = ['TSLA', \"DOGE\", \"AAPL\"]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPWoCaCjoNe-"
      },
      "source": [
        "4. Search for stock news using Google and Yahoo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7nj_WpYoSZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abcd16e-790c-4251-e953-79d6337228f8"
      },
      "source": [
        "def search_for_stock_news_url(ticker):\n",
        "  search_url = \"https://www.google.com/search?q=yahoo+finance+{}&tbm=nws\".format(ticker)\n",
        "  r= requests.get(search_url)\n",
        "  soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "  atags = soup.findAll('a')\n",
        "  hrefs = [link['href'] for link in atags]\n",
        "  return hrefs\n",
        "\n",
        "print('Searching for stock news for: ', monitored_tickers)  \n",
        "raw_url = {ticker:search_for_stock_news_url(ticker) for ticker in monitored_tickers}"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching for stock news for:  ['TSLA', 'DOGE', 'AAPL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiBdAfZxAYQl"
      },
      "source": [
        "5. Clean raw urls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEGTTrn6AafD"
      },
      "source": [
        "import re"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bN-kG29AdgT",
        "outputId": "1530d082-e4e4-4952-d11b-a48c80434372"
      },
      "source": [
        "exclude_list = [\"google.com/\"]\n",
        "def strip_unwanted_urls(urls, exclude_list):\n",
        "  val = []\n",
        "  for url in urls:\n",
        "    if \"https://\" in url and not any(exclude_word in url for exclude_word in exclude_list):\n",
        "      res = re.findall(r'https?://\\S+', url)[0].split('&')[0]\n",
        "      val.append(res)\n",
        "  return list(set(val))\n",
        "\n",
        "print('Cleaning URLs.')  \n",
        "cleaned_urls = {ticker:strip_unwanted_urls(raw_url[ticker], exclude_list) for ticker in monitored_tickers} "
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning URLs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR7SMrnHee-B"
      },
      "source": [
        "6. Get Articles from URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxPAYAOyKQJP",
        "outputId": "9b5ea7c1-f3a6-44e1-aa44-9f10085bf328"
      },
      "source": [
        "def scrape_and_process_urls(urls):\n",
        "  articles = []\n",
        "  for url in urls:\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    paragraphs = soup.findAll('p')\n",
        "    text = [paragraph.text for paragraph in paragraphs]\n",
        "    words = \" \".join(text).split(\" \")[:350]\n",
        "    article = \" \".join(words)\n",
        "    articles.append(article)\n",
        "  return articles\n",
        "\n",
        "print('Scraping news links.')\n",
        "url_articles = {ticker:scrape_and_process_urls(cleaned_urls[ticker]) for ticker in monitored_tickers} "
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping news links.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U64cBWNELBFG"
      },
      "source": [
        "7. Summarize all the articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNJSkjhuLxDN",
        "outputId": "d70c850f-b658-4313-fa5b-3666077c0788"
      },
      "source": [
        "def summarize_articles(articles):\n",
        "  summaries = []\n",
        "  for article in articles:\n",
        "    input_ids = tokenizer.encode(article, return_tensors=\"pt\", truncation=True)\n",
        "    output = model.generate(input_ids,max_length=50, num_beams=5, early_stopping=True)\n",
        "    summary = tokenizer.decode(output[0],skip_special_tokens=True)\n",
        "    summaries.append(summary)\n",
        "  return summaries\n",
        "\n",
        "print(\"Getting summaries of links.\")\n",
        "url_summaries = {ticker:summarize_articles(url_articles[ticker]) for ticker in monitored_tickers} "
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting summaries of links.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGrKZaNqVaSf"
      },
      "source": [
        "8. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Huxz-hVXKw"
      },
      "source": [
        "from transformers import pipeline\n",
        "sentiment = pipeline(\"sentiment-analysis\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kE0QQhzWi6z",
        "outputId": "3c727711-16b0-4030-a0c6-e8cc0b043446"
      },
      "source": [
        "print('Analysing and scoring the summaries.')\n",
        "scores = {ticker: sentiment(url_summaries[ticker]) for ticker in monitored_tickers}"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analysing and scoring the summaries.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O02pvmCVX7Re"
      },
      "source": [
        "9. Exporting as CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhX4wYFLX65h",
        "outputId": "6cf64549-bb85-412a-e1d5-4883ba6b13d9"
      },
      "source": [
        "def create_output_array(tickers, summaries, scores, urls):\n",
        "  output = []\n",
        "  for ticker in tickers:\n",
        "    for i in range(len(summaries[ticker])):\n",
        "      data = [\n",
        "              ticker,\n",
        "              summaries[ticker][i],\n",
        "              scores[ticker][i]['label'],\n",
        "              scores[ticker][i]['score'],\n",
        "              urls[ticker][i]\n",
        "              ]\n",
        "      output.append(data)\n",
        "  return output\n",
        "\n",
        "print('Exporting results.')\n",
        "final_output = create_output_array(monitored_tickers, url_summaries, scores, cleaned_urls)\n",
        "final_output.insert(0,['Ticker', 'Summary', 'Label','Score','Url'])\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exporting results.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoRVtoVvbhjy",
        "outputId": "cb566f4f-a1ef-487d-b13b-125704031b35"
      },
      "source": [
        "import csv\n",
        "with open('assetSummaries.csv', mode='w',newline='') as f:\n",
        "  csv_writer = csv.writer(f, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  csv_writer.writerows(final_output)\n",
        "print(\"Data saved as 'assetSummaries.csv' file!\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data saved as 'assetSummaries.csv' file!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}